# Lesson 6.2: Autonomous Systems, Deepfakes & Misinformation

## üéØ Lesson Objectives
By the end of this lesson, students should be able to:
* **Define** autonomous systems, deepfakes, misinformation, and disinformation.
* **Identify** ethical concerns related to automation and synthetic media.
* **Analyze** real-life case studies (Uber, Cambridge Analytica, and Kenyan health myths).
* **Propose** accountability frameworks for ICT professionals.

---

## 1. Understanding Key Concepts

| Concept | Definition | Example |
| :--- | :--- | :--- |
| **Autonomous Systems** | Systems that make decisions without direct human input. | Self-driving cars (Tesla), trading bots. |
| **Deepfakes** | AI-generated media that convincingly imitates real people. | Synthetic videos of political leaders. |
| **Misinformation** | False info shared **without** harmful intent. | Forwarding a fake cure on WhatsApp by mistake. |
| **Disinformation** | False info shared **with** intent to deceive. | State-sponsored propaganda during elections. |

---

## 2. Ethical Concerns and Case Studies

### A. Autonomous Systems: The Accountability Gap
The primary ethical dilemma is **Responsibility**: If a machine makes a mistake, who is liable?



**Case Study: Uber Self-Driving Fatality (2018)**
* **The Event:** A self-driving Uber struck a pedestrian in Arizona. The AI detected her but classified her as an "unknown object" and didn't brake.
* **Ethical Failure:** The safety driver was distracted, and the system was tuned to ignore "false positives," leading to a fatal delay in reaction.
* **Lesson:** Blind trust in automation is dangerous. **Human-in-the-loop** design is mandatory for high-stakes systems.

---

### B. Deepfakes: The Death of Reality
Deepfakes use Generative Adversarial Networks (GANs) to create "synthetic reality," leading to identity theft and social manipulation.

**Case Study: Kenyan Political Deepfakes (2024)**
* **The Incident:** AI-generated clips of local politicians circulated on TikTok during policy debates. These videos used voice cloning to make them appear authentic.
* **Impact:** Public confusion and the erosion of trust in digital evidence (the "Liar's Dividend").
* **ICT Duty:** Developers must implement **digital watermarking** and metadata that identifies AI-generated content.

---

### C. Misinformation vs. Disinformation (The Kenyan Context)
While misinformation is often a "mistake," disinformation is a "weapon."

**1. COVID-19 WhatsApp Myths (Misinformation)**
* **Scenario:** Messages claiming "ginger and lemon" were proven cures for COVID-19 spread rapidly in Kenya.
* **Harm:** People delayed seeking professional medical help, believing they were safe.

**2. Cambridge Analytica (Disinformation)**
* **Scenario:** During the **2017 Kenya General Election**, personal data was harvested to create "psychographic profiles."
* **Action:** Targeted disinformation ads were used to trigger fear and manipulate voter behavior.
* **Lesson:** ICT professionals handling big data must ensure **informed consent** and protect systems from being used for psychological manipulation.

---

## 3. Summary Table of Ethical Risks

| Technology | Key Ethical Risk | Case Example | Professional Lesson |
| :--- | :--- | :--- | :--- |
| **Autonomous Systems** | Accountability | Uber Fatality | Maintain audit logs and human oversight. |
| **Deepfakes** | Consent/Identity | Pornography Lawsuits | Advocate for verification tools (e.g., Blockchain). |
| **Misinformation** | Public Safety | COVID-19 Myths | Design fact-checking into social platforms. |
| **Disinformation** | Democracy | Cambridge Analytica | Enforce strict data ethics and transparency. |

---

## 4. Professional Responsibilities for ICT Practitioners
In the age of synthetic media, you are the **Guardians of Truth**:

1.  **Ethics-by-Design:** Build safeguards into the system *before* launch (e.g., self-correcting logic in autonomous cars).
2.  **Verification:** Implement tools to verify the source of digital content (Digital Signatures).
3.  **Accountability:** Ensure every autonomous decision is logged in an **Immutable Audit Trail**.
4.  **Digital Literacy:** Educate your clients and the public on how to spot deepfakes and fake news.
5.  **Compliance:** Adhere to **Kenya‚Äôs Computer Misuse and Cybercrimes Act (2018)** which criminalizes the publication of false data.

---

## üó£Ô∏è Discussion & Activity

### Discussion Questions
1. If a self-driving car crashes in Nairobi, should the **software engineer** go to jail, or the **car owner**?
2. Should it be a crime to create a "parody" deepfake of a celebrity without their permission?

### Group Activity: "Fact-Check Challenge" (10 mins)
* **Scenario:** A viral video shows a local bank "collapsing," causing a panic run. 
* **Task:** As the bank's ICT Security Team, list **three technical steps** you would take to prove the video is a deepfake and restore public trust.

---
¬© 2026 ICT Ethics & Law Module. Distributed under the MIT License.